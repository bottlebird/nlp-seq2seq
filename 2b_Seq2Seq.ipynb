{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2b_Seq2Seq.ipynb","provenance":[],"collapsed_sections":["qf8Oc9a_ocC_"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FU7xWiY6TyWS","colab_type":"code","colab":{}},"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n","rm -rf 6864-hw2b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uie6jWpIhg_B","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"5AyMA9rK1Rhf","colab_type":"code","colab":{}},"source":["import os\n","os.makedirs(\"6864-hw2b\", exist_ok=True)\n","import sys\n","sys.path.append(\"/content/6864-hw2b\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BL1IfnRdPdsl","colab_type":"code","outputId":"e9af8069-61b3-49b4-eb18-4a831cd3c8ff","executionInfo":{"status":"ok","timestamp":1586148811764,"user_tz":240,"elapsed":4651,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!pip install sacrebleu"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.4.6)\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (0.996.5)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (1.6.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5fOArV2r9Piz","colab_type":"text"},"source":["# **Part 3: Sequence-to-Sequence Model**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mA9JfWiK9eoL","colab_type":"text"},"source":["In this lab, you will explore RNN-based sequence-to-sequence (seq2seq) models to perform machine translation (MT). We will use a Vietnamese-English dataset from IWSLT'15. The task is to translate a Vietnamese sentence into English.\n","\n","The lab is divided into two parts. The first part is to implement a vanilla seq2seq architecture without attention. In the second part you will implement your favorite attention mechanism (doesn't have to come from lecture) and add it to your vanilla seq2seq model. We will provide the training and testing scripts (trust me, the decoding/testing is actually the hardest part :P), so you will mainly just have to focus on implementing the models (I say *mainly* because you still might need to modify the testing script, depending on which attention method you use and how you implement it).\n"]},{"cell_type":"markdown","metadata":{"id":"zDJjmvZfHV_l","colab_type":"text"},"source":["## **Section 1: Data Preprocessing**\n","\n","No need to write any code in this section. But you are encouraged to test with this part to understand the data.\n","\n","First, we download the dataset and place it under current directory."]},{"cell_type":"code","metadata":{"id":"02RioHPryvOz","colab_type":"code","outputId":"e11f62f6-3521-4756-adfe-252236f0ec50","executionInfo":{"status":"ok","timestamp":1586148848089,"user_tz":240,"elapsed":33071,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# Download data\n","!wget -nv -O /content/6864-hw2b/train.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n","!wget -nv -O /content/6864-hw2b/train.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n","!wget -nv -O /content/6864-hw2b/tst2013.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n","!wget -nv -O /content/6864-hw2b/tst2013.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n","!wget -nv -O /content/6864-hw2b/vocab.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n","!wget -nv -O /content/6864-hw2b/vocab.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-06 04:53:40 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en [13603614/13603614] -> \"/content/6864-hw2b/train.en\" [1]\n","2020-04-06 04:53:48 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi [18074646/18074646] -> \"/content/6864-hw2b/train.vi\" [1]\n","2020-04-06 04:53:52 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en [132264/132264] -> \"/content/6864-hw2b/tst2013.en\" [1]\n","2020-04-06 04:53:57 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi [183855/183855] -> \"/content/6864-hw2b/tst2013.vi\" [1]\n","2020-04-06 04:54:02 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/6864-hw2b/vocab.en\" [1]\n","2020-04-06 04:54:06 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi [46767/46767] -> \"/content/6864-hw2b/vocab.vi\" [1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ogFESHAf-6MY","colab_type":"text"},"source":["Next, we do some simple data preprocessing and show some data statistics."]},{"cell_type":"code","metadata":{"id":"OfkQGqV30hgC","colab_type":"code","outputId":"0498375d-cd3d-422b-a7bc-96ac73719116","executionInfo":{"status":"ok","timestamp":1586148850345,"user_tz":240,"elapsed":33404,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":452}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def read_sentence_file(filename):\n","  sentences_list = []\n","  with open(filename, \"r\") as f:\n","    for line in f:\n","      sentences_list.append(line.strip().split())\n","  return sentences_list\n","\n","def read_vocab_file(filename):\n","  with open(filename, \"r\") as f:\n","    return [line.strip() for line in f]\n","\n","\n","src_vocab_set = read_vocab_file(os.path.join(\"/content/6864-hw2b\", \"vocab.vi\"))\n","trg_vocab_set = read_vocab_file(os.path.join(\"/content/6864-hw2b\", \"vocab.en\"))\n","\n","train_src_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                           \"train.vi\"))\n","train_trg_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                           \"train.en\"))\n","assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n","\n","test_src_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                          \"tst2013.vi\"))\n","test_trg_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                          \"tst2013.en\"))\n","assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n","\n","\n","MAX_SENT_LENGTH = 48\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n","\n","# We only keep sentences that do not exceed 48 words, so that later when we\n","# add <s> and </s> to a sentence it still won't exceed 50 words.\n","def filter_data(src_sentences_list, trg_sentences_list, max_len):\n","  new_src_sentences_list, new_trg_sentences_list = [], []\n","  for src_sent, trg_sent in zip(src_sentences_list, trg_sentences_list):\n","    if (len(src_sent) <= max_len and len(trg_sent) <= max_len\n","        and len(src_sent) > 0 and len(trg_sent)) > 0:\n","      new_src_sentences_list.append(src_sent)\n","      new_trg_sentences_list.append(trg_sent)\n","  return new_src_sentences_list, new_trg_sentences_list\n","\n","train_src_sentences_list, train_trg_sentences_list = filter_data(\n","    train_src_sentences_list, train_trg_sentences_list, max_len=MAX_SENT_LENGTH)\n","test_src_sentences_list, test_trg_sentences_list = filter_data(\n","    test_src_sentences_list, test_trg_sentences_list, max_len=MAX_SENT_LENGTH)\n","\n","# We take 10% of training data as validation set.\n","num_val = int(len(train_src_sentences_list) * 0.1)\n","val_src_sentences_list = train_src_sentences_list[:num_val]\n","val_trg_sentences_list = train_trg_sentences_list[:num_val]\n","train_src_sentences_list = train_src_sentences_list[num_val:]\n","train_trg_sentences_list = train_trg_sentences_list[num_val:]\n","\n","# Show some data stats\n","print(\"Number of training (src, trg) sentence pairs: %d\" %\n","      len(train_src_sentences_list))\n","print(\"Number of validation (src, trg) sentence pairs: %d\" %\n","      len(val_src_sentences_list))\n","print(\"Number of testing (src, trg) sentence pairs: %d\" %\n","      len(test_src_sentences_list))\n","src_vocab_set = ['<pad>'] + src_vocab_set\n","trg_vocab_set = ['<pad>'] + trg_vocab_set\n","print(\"Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\" %\n","      len(src_vocab_set))\n","print(\"Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\" %\n","      len(trg_vocab_set))\n","\n","length = [len(sent) for sent in train_src_sentences_list]\n","print('Training sentence avg. length: %d ' % np.mean(length))\n","print('Training sentence length at 95-percentile: %d' %\n","      np.percentile(length, 95))\n","print('Training sentence length distribution '\n","      '(x-axis is length range and y-axis is count):\\n')\n","plt.hist(length, bins=5)\n","plt.show()\n","\n","print('Example Vietnamese input: ' + str(train_src_sentences_list[0]))\n","print('Its target English output: ' + str(train_trg_sentences_list[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of training (src, trg) sentence pairs: 108748\n","Number of validation (src, trg) sentence pairs: 12083\n","Number of testing (src, trg) sentence pairs: 1139\n","Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 7710\n","Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 17192\n","Training sentence avg. length: 20 \n","Training sentence length at 95-percentile: 42\n","Training sentence length distribution (x-axis is length range and y-axis is count):\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Example Vietnamese input: ['Adam', 'Sadowsky', 'dàn', 'dựng', '1', 'video', 'âm', 'nhạc', 'hiện', 'tượng', '.']\n","Its target English output: ['Adam', 'Sadowsky', ':', 'How', 'to', 'engineer', 'a', 'viral', 'music', 'video']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J2x0lhVm_Yxx","colab_type":"text"},"source":["Here we define a class called `MTDataset`. It is built on top of the efficient data loader API provided in PyTorch. See Section 5 for explanation."]},{"cell_type":"code","metadata":{"id":"muwDBzXM5ijT","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils import data\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"   # use gpu whenever you can!\n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","\n","# These IDs are reserved.\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","\n","\n","class MTDataset(data.Dataset):\n","  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n","               sampling=1.):\n","    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n","    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n","\n","    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","\n","    self.src_vocabs = src_vocabs\n","    self.trg_vocabs = trg_vocabs\n","\n","    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n","    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n","    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n","    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n","\n","  def __len__(self):\n","    return len(self.src_sentences)\n","\n","  def __getitem__(self, index):\n","    src_sent = self.src_sentences[index]\n","    src_len = len(src_sent) + 2   # add <s> and </s> to each sentence\n","    src_id = []\n","    for w in src_sent:\n","      if w not in self.src_vocabs:\n","        w = '<unk>'\n","      src_id.append(self.src_v2id[w])\n","    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n","              (self.max_src_seq_length - src_len))\n","\n","    trg_sent = self.trg_sentences[index]\n","    trg_len = len(trg_sent) + 2\n","    trg_id = []\n","    for w in trg_sent:\n","      if w not in self.trg_vocabs:\n","        w = '<unk>'\n","      trg_id.append(self.trg_v2id[w])\n","    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n","              (self.max_trg_seq_length - trg_len))\n","\n","    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kb5gQEp7oVdi","colab_type":"text"},"source":["## **Section 2: Encoder**\n","\n","Seq2seq consists of an Encoder RNN and a decoder RNN. In a vanilla seq2seq model where there is no attention mechanism between encoder and decoder, the encoder aims to compress the information contained in the entire input sequence into a single vector and pass it to decoder.\n","\n","We start with implementing the encoder, which is just a simple RNN. We use a GRU here, but feel free to try other cell types."]},{"cell_type":"code","metadata":{"id":"dnwVGDVkoPt0","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    \n","    # Note: for lab writeup question #4, you can directly change `num_layers`\n","    # and `bidirectional` here to enable deep/bidirectional RNNs. However, you\n","    # will also need to modify some parts in the rest of the code accordingly.\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    packed_inputs = pack_padded_sequence(inputs,lengths,batch_first=True,enforce_sorted=False)\n","    outputs, finals = self.rnn(packed_inputs) #output(128,50,256), finals(1,128,256)\n","    outputs = pad_packed_sequence(packed_inputs, batch_first=True, total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","\n","    return outputs, finals"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Oz3Kc4QKyEP","colab_type":"text"},"source":["## **Section 3: Decoder**\n","\n","Here you will implement a decoder RNN that uses encoder's last hidden state to initialize its initial hidden state."]},{"cell_type":"code","metadata":{"id":"JYT0BlfYUJXj","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder without attention.\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    \n","  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of target\n","          sentences (for teacher-forcing during training).\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the raw\n","          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n","          We will convert it later in a `Generator` below.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the last decoder hidden state.\n","    \"\"\"\n","\n","    # The maximum number of steps to unroll the RNN.\n","    if max_len is None:\n","      max_len = inputs.size(1)\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    outputs = None\n","    \n","    for i in range(max_len):\n","      input=input[:,i,:]\n","      inputs=F.relu(inputs) #input (128,49,256); hiddens(1,128,256)\n","    outputs, hidden = self.rnn(inputs,hidden)\n","\n","    return hidden, outputs\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","    state.\"\"\"\n","    ### Your code here!\n","    decoder_init_hiddens=encoder_finals\n","\n","    return decoder_init_hiddens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AH0VdHE2_x1k","colab_type":"text"},"source":["Define the high level encoder-decoder class to wrap up sub-models, including encoder, decoder, generator, and src/trg embeddings."]},{"cell_type":"code","metadata":{"id":"nNBaAYB_oHxG","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","  \"\"\"A standard Encoder-Decoder architecture without attention.\n","  \"\"\"\n","  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n","    \"\"\"\n","    Inputs:\n","      - `encoder`: an `Encoder` object.\n","      - `decoder`: an `Decoder` object.\n","      - `src_embed`: an nn.Embedding object representing the lookup table for\n","          input (source) sentences.\n","      - `trg_embed`: an nn.Embedding object representing the lookup table for\n","          output (target) sentences.\n","      - `generator`: a `Generator` object. Essentially a linear mapping. See\n","          the next code cell.\n","    \"\"\"\n","    super(EncoderDecoder, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.trg_embed = trg_embed\n","    self.generator = generator\n","\n","  def forward(self, src_ids, trg_ids, src_lengths):\n","    \"\"\"Take in and process masked source and target sequences.\n","\n","    Inputs:\n","      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of source sentences of word ids.\n","      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of target sentences of word ids.\n","      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n","        sequence length of `src_ids`.\n","\n","    Returns the decoder outputs, see the above cell.\n","    \"\"\"\n","    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n","    del encoder_hiddens   # unused\n","    return self.decode(encoder_finals, trg_ids[:, :-1])\n","\n","  def encode(self, src_ids, src_lengths):\n","    return self.encoder(self.src_embed(src_ids), src_lengths)\n","    \n","  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n","    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M06QOTbCALGy","colab_type":"text"},"source":["It simply projects the pre-output layer (x in the forward function below) to obtain the output layer, so that the final dimension is the target vocabulary size."]},{"cell_type":"code","metadata":{"id":"LaHdVcF1KPmd","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size):\n","    super(Generator, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","  def forward(self, x):\n","    return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qf8Oc9a_ocC_","colab_type":"text"},"source":["## **Section 4: Attention-Based Decoder**\n","\n","Now it's time to add some attention to the decoder. You can implement any attention mechanism you want.\n"]},{"cell_type":"code","metadata":{"id":"iZq2NImAoY1C","colab_type":"code","colab":{}},"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An attention-based RNN decoder.\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention=None, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n","        - `attention`: this is your self-defined Attention object. You can\n","            either define an individual class for your Attention and pass it\n","            here or leave `attention` as None and just implement everything\n","            here.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","\n","    ### Your code here!\n","    self.dropout = nn.Dropout(p=dropout)\n","    self.attn = nn.Linear(hidden_size + input_size, hidden_size)\n","    self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n","    self.out = nn.Linear(hidden_size, hidden_size)\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","    #self.softmax=nn.LogSoftmax(dim=1)\n","\n","    \n","  def forward(self, inputs, encoder_hiddens, encoder_finals, #src_mask, trg_mask,\n","              hidden=None, max_len=None):\n","    ##inputs(128,49,256); encoder_hiddens(128,50,256); encoder_finals(1,128,256)\n","\n","    \"\"\"Unroll the decoder one step at a time.\n","    \n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of target\n","          sentences (for teacher-forcing during training).\n","      - `encoder_hiddens`: a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the encoder\n","          outputs for each decoding step to attend to. \n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n","          representing the mask for source sentences.\n","      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n","          representing the mask for target sentences.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `outputs`: (same as in Decoder) a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the raw\n","          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n","          representing the last decoder hidden state.\n","    \"\"\"\n","\n","    # The maximum number of steps to unroll the RNN.\n","    if max_len is None:\n","      max_len = inputs.size(-1) #max_len=256\n","\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    outputs = None\n","    ### Your code here!\n","    #inputs = self.dropout(inputs)\n","    #attention weight: take prev_hidden and embedded encoder inputs\n","    #then, softmax attn weights -> apply on encoder inputs -> combine (layer)\n","    \n","    inputs = self.dropout(inputs)\n","    inputs = inputs.permute(0,1,2).size()\n","    for input_1 in inputs: ##inputs(128,49,256), input_1(49,256)\n","      attn = torch.cat((input_1, hidden[0]), 1)\n","      attn_weights = F.softmax(attn, dim=1)\n","      attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                               encoder_outputs.unsqueeze(0))\n","      output = torch.cat((inputs[0], attn_applied[0]), 1)\n","      output = self.attn_combine(output).unsqueeze(0)\n","      output=F.relu(output)\n","      output, hidden = self.rnn(output, hidden)\n","      return hidden, output\n","    #output = F.log_softmax(self.out(output[0]), dim=1)\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","    state.\"\"\"\n","    decoder_init_hiddens = None\n","    ### Your code here!\n","    #print(\"encoder_finals\",encoder_finals)\n","    #print(\"encoder_finals [:-1]\", encoder_finals[:-1])\n","    decoder_init_hiddens=encoder_finals\n","\n","    return decoder_init_hiddens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHfPNAowKCJ-","colab_type":"text"},"source":["TEST!!"]},{"cell_type":"code","metadata":{"id":"acK4y8h1JqbJ","colab_type":"code","outputId":"21de4d7a-35c5-4b17-b2ec-034572bcbc1f","executionInfo":{"status":"error","timestamp":1586149802629,"user_tz":240,"elapsed":2258,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":354}},"source":["import torch.nn.functional as F\n","\n","embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n","hidden_size = 256  # RNN hidden size.\n","dropout = 0.2\n","\n","encoder=Encoder(embed_size, hidden_size, dropout=dropout)\n","decoder=AttentionDecoder(embed_size, hidden_size)\n","src_embed=nn.Embedding(len(src_vocab_set), embed_size)\n","trg_embed=nn.Embedding(len(trg_vocab_set), embed_size)\n","generator=Generator(hidden_size, len(trg_vocab_set))\n","\n","src_ids, src_lengths, trg_ids, trg_lengths = next(iter(train_data_loader))\n","\n","encoder_hiddens, encoder_finals = encoder(src_embed(src_ids), src_lengths)\n","#output, hidden = decoder(trg_embed(trg_ids), encoder_hiddens, encoder_final)\n","inputs=trg_embed(trg_ids)\n","\n","max_len = inputs.size(-1) #max_len=256\n","hidden=None\n","\n","if hidden is None:\n","  hidden = encoder_finals\n","\n","outputs = None\n","\n","inputs = F.dropout(inputs)\n","#inputs = inputs.permute(0,1,2).size()\n","\n","print(inputs.s)\n","print(hidden.size())\n","attn = torch.cat((inputs, hidden), 1)\n","attn_weights = F.softmax(attn, dim=1)\n","attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                         encoder_outputs.unsqueeze(0))\n","output = torch.cat((inputs[0], attn_applied[0]), 1)\n","output = self.attn_combine(output).unsqueeze(0)\n","output=F.relu(output)\n","output, hidden = self.rnn(output, hidden)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["output size torch.Size([128, 50, 256])\n","finals size torch.Size([1, 128, 256])\n","3\n","torch.Size([1, 128, 256])\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-8837c5e6cb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got torch.Size"]}]},{"cell_type":"markdown","metadata":{"id":"DTUXxJWPPQ9W","colab_type":"text"},"source":["Similarly, we use a `EncoderAttentionDecoder` class to wrap up all encoder, decoder, src/trg embeddings, and generator. You can take the `EncoderDecoder` class as a reference."]},{"cell_type":"code","metadata":{"id":"mghIa6XzubZL","colab_type":"code","colab":{}},"source":["class EncoderAttentionDecoder(nn.Module):\n","  \"\"\"A Encoder-Decoder architecture with attention.\n","  \"\"\"\n","  def __init__(self, encoder, decoder, src_embed , trg_embed, generator):\n","    \"\"\"\n","    Inputs:\n","      - `encoder`: an `Encoder` object.\n","      - `decoder`: an `AttentionDecoder` object.\n","      - `src_embed`: an nn.Embedding object representing the lookup table for\n","          input (source) sentences.\n","      - `trg_embed`: an nn.Embedding object representing the lookup table for\n","          output (target) sentences.\n","      - `generator`: a `Generator` object. Essentially a linear mapping. See\n","          the next code cell.\n","    \"\"\"\n","    super(EncoderAttentionDecoder, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.trg_embed = trg_embed\n","    self.generator = generator\n","\n","  def forward(self, src_ids, trg_ids, src_lengths):\n","    \"\"\"Take in and process masked source and target sequences.\n","\n","    Inputs:\n","      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of source sentences of word ids.\n","      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of target sentences of word ids.\n","      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n","        sequence length of `src_ids`.\n","\n","    Returns the decoder outputs, see the above cell.\n","    \"\"\"\n","    ### Your code here!\n","    # You can refer to `EncoderDecoder` and extend from it.\n","    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n","    #del encoder_hiddens   # unused\n","    return self.decode(trg_ids[:, :-1], encoder_hiddens, encoder_finals)\n","\n","  def encode(self, src_ids, src_lengths):\n","    return self.encoder(self.src_embed(src_ids), src_lengths)\n","\n","  def decode(self, trg_ids, encoder_hiddens,decoder_hidden=None):\n","    print(self.trg_embed(trg_ids))\n","    return self.decoder(self.trg_embed(trg_ids), encoder_hiddens, decoder_hidden)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VIpNlKtK8l_","colab_type":"text"},"source":["## **Section 5: Training and Testing**\n","\n","We provide training and testing scripts here. You might need to adapt them to fit your model implementation."]},{"cell_type":"markdown","metadata":{"id":"I38IFq48BKa5","colab_type":"text"},"source":["Apply the dataloader to the MT dataset. Dataloader provides a convenient way to iterate through the whole dataset."]},{"cell_type":"code","metadata":{"id":"cJrXO7nCjzBP","colab_type":"code","colab":{}},"source":["batch_size = 128\n","\n","# You can try on a smaller training set by setting a smaller `sampling`.\n","train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n","                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n","train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n","                                    num_workers=8, shuffle=True)\n","\n","val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n","                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n","val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n","                                  shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWaiu7wNBX7x","colab_type":"text"},"source":["The main functions for training, here we use perplexity to evaluate the performance of the model. Although we provide the training scripts here, we strongly encoureage you to go through and understand the procedure."]},{"cell_type":"code","metadata":{"id":"AXGa-L1qp13q","colab_type":"code","colab":{}},"source":["import math\n","\n","\n","class SimpleLossCompute:\n","  \"\"\"A simple loss compute and train function.\"\"\"\n","\n","  def __init__(self, generator, criterion, opt=None):\n","    self.generator = generator\n","    self.criterion = criterion\n","    self.opt = opt\n","\n","  def __call__(self, x, y, norm):\n","    x = self.generator(x)\n","    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                          y.contiguous().view(-1))\n","    loss = loss / norm\n","\n","    if self.opt is not None:  # training mode\n","      loss.backward()          \n","      self.opt.step()\n","      self.opt.zero_grad()\n","\n","    return loss.data.item() * norm\n","\n","\n","def run_epoch(data_loader, model, loss_compute, print_every):\n","  \"\"\"Standard Training and Logging Function\"\"\"\n","\n","  total_tokens = 0\n","  total_loss = 0\n","\n","  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n","    # We define some notations here to help you understand the loaded tensor\n","    # shapes:\n","    #   `B`: batch size\n","    #   `T`: max sequence length of source sentences\n","    #   `L`: max sequence length of target sentences; due to our preprocessing\n","    #        in the beginning, `L` == `T` == 50\n","    # An example of `src_ids_BxT` (when B = 2):\n","    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n","    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n","    # The corresponding `src_lengths_B` would be [47, 49].\n","    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n","\n","    src_ids_BxT = src_ids_BxT.to(device)\n","    src_lengths_B = src_lengths_B.to(device)\n","    trg_ids_BxL = trg_ids_BxL.to(device)\n","    del trg_lengths_B   # unused\n","\n","    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n","\n","    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n","                        norm=src_ids_BxT.size(0))\n","    total_loss += loss\n","    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n","\n","  return math.exp(total_loss / float(total_tokens))\n","\n","\n","def train(model, num_epochs, learning_rate, print_every):\n","  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","  # computing the loss.\n","  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n","  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # Keep track of dev ppl for each epoch.\n","  dev_ppls = []\n","\n","  for epoch in range(num_epochs):\n","    print(\"Epoch\", epoch)\n","\n","    model.train()\n","    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n","                          loss_compute=SimpleLossCompute(model.generator,\n","                                                         criterion, optim),\n","                          print_every=print_every)\n","        \n","    model.eval()\n","    with torch.no_grad():      \n","      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n","                          loss_compute=SimpleLossCompute(model.generator,\n","                                                         criterion, None),\n","                          print_every=print_every)\n","      print(\"Validation perplexity: %f\" % dev_ppl)\n","      dev_ppls.append(dev_ppl)\n","        \n","  return dev_ppls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1A8VvkcICT60","colab_type":"text"},"source":["The main function to perform training. First let's train the vanilla seq2seq model."]},{"cell_type":"code","metadata":{"id":"pZ0t1hXAIHtO","colab_type":"code","outputId":"b65eb3fb-38be-44b7-8157-3351e45df12e","executionInfo":{"status":"error","timestamp":1586126088796,"user_tz":240,"elapsed":10281,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":429}},"source":["# Hyperparameters for contructing the encoder-decoder model.\n","\n","embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n","hidden_size = 256  # RNN hidden size.\n","dropout = 0.2\n","\n","pure_seq2seq = EncoderDecoder(\n","  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n","  decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n","  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n","  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n","  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n","\n","# Start training. The returned `dev_ppls` is a list of dev perplexity for each\n","# epoch.\n","pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n","                      print_every=100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-ffac4e7ef14a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msrc_embed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtrg_embed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_vocab_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Start training. The returned `dev_ppls` is a list of dev perplexity for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Resets _flat_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"fRsfDg0wCa7U","colab_type":"text"},"source":["Plot the perplexity graph."]},{"cell_type":"code","metadata":{"id":"CTApnlT53YvT","colab_type":"code","outputId":"4fc5eba0-8943-4b32-b47d-a1f9435541da","executionInfo":{"status":"ok","timestamp":1585624279660,"user_tz":240,"elapsed":421,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["def plot_perplexity(perplexities):\n","  \"\"\"plot perplexities\"\"\"\n","  plt.title(\"Perplexity per Epoch\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Perplexity\")\n","  plt.plot(perplexities)\n","\n","plot_perplexity(pure_dev_ppls)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWUbBLyKSZRT8IiJZ5v8DaAnEpXzjSWoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"PjhKXDyvIk4l","colab_type":"text"},"source":["Now, let's train the seq2seq model with attention."]},{"cell_type":"code","metadata":{"id":"onQxDU-aGa0t","colab_type":"code","outputId":"d56dda04-f635-4c7e-ccb8-b64d4de3af2c","executionInfo":{"status":"error","timestamp":1586126211521,"user_tz":240,"elapsed":2231,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["attn_seq2seq = EncoderAttentionDecoder(\n","  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n","  decoder=AttentionDecoder(embed_size, hidden_size,\n","                  attention=None, dropout=dropout),\n","  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n","  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n","  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n","\n","attn_dev_ppls = train(attn_seq2seq, num_epochs=10, learning_rate=1e-3,\n","                      print_every=100)\n","\n","plot_perplexity(attn_dev_ppls)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["output size torch.Size([128, 50, 256])\n","finals size torch.Size([1, 128, 256])\n","tensor([[[ 1.6714, -0.0309,  1.4718,  ...,  0.4928, -0.8570,  0.4631],\n","         [ 0.1660, -0.9246, -2.6228,  ..., -0.2234, -0.5327,  1.2713],\n","         [-1.0331, -1.0384, -0.2765,  ..., -0.4724,  0.9077,  0.9004],\n","         ...,\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747]],\n","\n","        [[ 1.6714, -0.0309,  1.4718,  ...,  0.4928, -0.8570,  0.4631],\n","         [-0.2838,  0.3135,  0.0588,  ...,  0.8472,  0.7299, -1.1717],\n","         [-2.1471, -0.3221,  1.4298,  ...,  0.3468,  1.4504,  1.0553],\n","         ...,\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747]],\n","\n","        [[ 1.6714, -0.0309,  1.4718,  ...,  0.4928, -0.8570,  0.4631],\n","         [ 0.9453,  0.2913,  1.5790,  ..., -1.4072,  0.3465, -0.5721],\n","         [ 0.1660, -0.9246, -2.6228,  ..., -0.2234, -0.5327,  1.2713],\n","         ...,\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747]],\n","\n","        ...,\n","\n","        [[ 1.6714, -0.0309,  1.4718,  ...,  0.4928, -0.8570,  0.4631],\n","         [ 0.2775,  1.6578, -1.1297,  ...,  0.7046, -0.2138,  1.3516],\n","         [ 0.4890, -1.9633, -0.1633,  ..., -1.1001, -0.2915,  0.1566],\n","         ...,\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747]],\n","\n","        [[ 1.6714, -0.0309,  1.4718,  ...,  0.4928, -0.8570,  0.4631],\n","         [ 0.0469, -0.7119, -0.8307,  ...,  2.6422, -1.0981, -1.3446],\n","         [ 0.3903,  1.4601, -2.3296,  ...,  1.4793,  1.6702,  1.4836],\n","         ...,\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747]],\n","\n","        [[ 1.6714, -0.0309,  1.4718,  ...,  0.4928, -0.8570,  0.4631],\n","         [ 0.3743, -0.4718,  1.2095,  ...,  0.7084,  1.3321,  0.3182],\n","         [ 0.0557, -0.7047,  1.4124,  ..., -0.1017,  0.9960,  0.4004],\n","         ...,\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747],\n","         [ 1.2543, -1.9881,  1.0454,  ..., -1.3076, -0.7196, -1.9747]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward>)\n","max_len 256\n","encoder_finals tensor([[[ 0.7897,  0.8848,  0.5348,  ...,  0.5319, -0.8984,  0.0626],\n","         [ 0.7897,  0.8848,  0.5348,  ...,  0.5320, -0.8984,  0.0623],\n","         [ 0.7897,  0.8848,  0.5348,  ...,  0.5319, -0.8984,  0.0626],\n","         ...,\n","         [ 0.7897,  0.8848,  0.5347,  ...,  0.5319, -0.8984,  0.0627],\n","         [ 0.7897,  0.8848,  0.5347,  ...,  0.5319, -0.8984,  0.0627],\n","         [ 0.7897,  0.8848,  0.5348,  ...,  0.5320, -0.8984,  0.0624]]],\n","       device='cuda:0', grad_fn=<CudnnRnnBackward>)\n","encoder_finals [:-1] tensor([], device='cuda:0', size=(0, 128, 256), grad_fn=<SliceBackward>)\n","inputs.size() torch.Size([128, 49, 256])\n","input[0] torch.Size([49, 256])\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-274e7879d155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m attn_dev_ppls = train(attn_seq2seq, num_epochs=10, learning_rate=1e-3,\n\u001b[0;32m---> 10\u001b[0;31m                       print_every=100)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplot_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_dev_ppls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-a1d0713b2e5c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, learning_rate, print_every)\u001b[0m\n\u001b[1;32m     77\u001b[0m                           loss_compute=SimpleLossCompute(model.generator,\n\u001b[1;32m     78\u001b[0m                                                          criterion, optim),\n\u001b[0;32m---> 79\u001b[0;31m                           print_every=print_every)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-a1d0713b2e5c>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_loader, model, loss_compute, print_every)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrg_lengths_B\u001b[0m   \u001b[0;31m# unused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_ids_BxT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_ids_BxL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-0e684e794f9d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_ids, trg_ids, src_lengths)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_finals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#del encoder_hiddens   # unused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_finals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-0e684e794f9d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, trg_ids, encoder_hiddens, decoder_hidden)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-18bcd9a77233>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_hiddens, encoder_finals, hidden, max_len)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input[0]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m       \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 128 and 49 in dimension 0 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71"]}]},{"cell_type":"markdown","metadata":{"id":"VMpL-_MwJOws","colab_type":"text"},"source":["This is the function used to decode the model output. For simplicity, we use greedy search here."]},{"cell_type":"code","metadata":{"id":"M1HTqYwy6-yL","colab_type":"code","colab":{}},"source":["# Note: you might have to modify this `greedy_decode` function to work with your\n","# `EncoderAttentionDecoder`.\n","\n","def greedy_decode(model, src_ids, src_lengths, max_len):\n","  \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n","\n","  with torch.no_grad():\n","    _, encoder_finals = model.encode(src_ids, src_lengths)\n","    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n","\n","  output = []\n","  hidden = None\n","\n","  for i in range(max_len):\n","    with torch.no_grad():\n","      hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n","      prob = model.generator(outputs[:, -1])\n","    _, next_word = torch.max(prob, dim=1)\n","    next_word = next_word.data.item()\n","    output.append(next_word)\n","    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n","\n","  output = np.array(output)\n","\n","  # Cut off everything starting from </s>.\n","  first_eos = np.where(output == EOS_INDEX)[0]\n","  if len(first_eos) > 0:\n","    output = output[:first_eos[0]]\n","  return output\n","  \n","\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_5go3VxJZKh","colab_type":"text"},"source":["Print the top 3 examples from the data loader by applying the greedy decoder."]},{"cell_type":"code","metadata":{"id":"cc3m4optFrb3","colab_type":"code","colab":{}},"source":["def print_examples(model, data_loader, n=3,\n","                   max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS, \n","                   src_vocab_set=src_vocab_set, trg_vocab_set=trg_vocab_set):\n","  \"\"\"Prints `n` examples. Assumes batch size of 1.\"\"\"\n","\n","  model.eval()\n","\n","  for i, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n","    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n","                           max_len=max_len)\n","\n","    # remove <s>\n","    src_ids = src_ids[0, 1:]\n","    trg_ids = trg_ids[0, 1:]\n","    # remove </s> and <pad>\n","    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n","    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n","\n","    print(\"Example #%d\" % (i + 1))\n","    print(\"Src : \", \" \".join(lookup_words(src_ids, vocab=src_vocab_set)))\n","    print(\"Trg : \", \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set)))\n","    print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab_set)))\n","    print()\n","\n","    if i == n - 1:\n","      break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7Hi4X0GJouK","colab_type":"text"},"source":["Here we use the validation dataset to print examples."]},{"cell_type":"code","metadata":{"id":"27thJIfreCgB","colab_type":"code","outputId":"03b7def4-de55-4135-fa53-7d62747937a8","executionInfo":{"status":"ok","timestamp":1585624292408,"user_tz":240,"elapsed":512,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n","                        val_trg_sentences_list, trg_vocab_set)\n","example_data_loader = data.DataLoader(val_set, batch_size=1, num_workers=1,\n","                                      shuffle=False)\n","\n","print_examples(pure_seq2seq, example_data_loader)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Example #1\n","Src :  Khoa học đằng sau một tiêu đề về khí hậu\n","Trg :  Rachel <unk> : The science behind a climate headline\n","Pred:  The <unk> of the Big Bang is about a climate crisis .\n","\n","Example #2\n","Src :  Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n","Trg :  I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n","Pred:  I want to tell you about how the world of the world are really aware of the world .\n","\n","Example #3\n","Src :  Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .\n","Trg :  <unk> that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n","Pred:  There &apos;s no problem when you look at the sea and the sky , or the <unk> , or the <unk> of the world .\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H5pTV5PqJtX4","colab_type":"text"},"source":["Compute the BLEU score. BLEU score is a standard measure to evaluate the translation results. For further details, you can refer to [this](https://en.wikipedia.org/wiki/BLEU) link."]},{"cell_type":"code","metadata":{"id":"6XGQYwHRPyne","colab_type":"code","outputId":"8cc6257c-02bb-4750-b452-e075ba8ce626","executionInfo":{"status":"error","timestamp":1585624365500,"user_tz":240,"elapsed":55152,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":249}},"source":["import sacrebleu\n","from tqdm import tqdm\n","\n","\n","def compute_BLEU(model, data_loader):\n","  bleu_score = []\n","\n","  model.eval()\n","  for src_ids, src_lengths, trg_ids, _ in tqdm(data_loader):\n","    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n","                           max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    # remove <s>\n","    src_ids = src_ids[0, 1:]\n","    trg_ids = trg_ids[0, 1:]\n","    # remove </s> and <pad>\n","    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n","    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n","\n","    pred = \" \".join(lookup_words(result, vocab=trg_vocab_set))\n","    targ = \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set))\n","\n","    bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n","\n","  return bleu_score\n","\n","\n","test_set = MTDataset(test_src_sentences_list, src_vocab_set,\n","                     test_trg_sentences_list, trg_vocab_set, sampling=1.)\n","test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers=8,\n","                                   shuffle=False)\n","\n","print('BLEU score: %f' % (np.mean(compute_BLEU(pure_seq2seq,\n","                                               test_data_loader))))\n","print('BLEU score: %f' % (np.mean(compute_BLEU(attn_seq2seq,\n","                                               test_data_loader))))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 1139/1139 [00:54<00:00, 20.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["BLEU score: 6.454834\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-f1a299aae127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m print('BLEU score: %f' % (np.mean(compute_BLEU(pure_seq2seq,\n\u001b[1;32m     33\u001b[0m                                                test_data_loader))))\n\u001b[0;32m---> 34\u001b[0;31m print('BLEU score: %f' % (np.mean(compute_BLEU(attn_seq2seq,\n\u001b[0m\u001b[1;32m     35\u001b[0m                                                test_data_loader))))\n","\u001b[0;31mNameError\u001b[0m: name 'attn_seq2seq' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"GbOgwJw_CkCW","colab_type":"text"},"source":["## **Part 3: Lab writeup**\n","\n","Your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n","\n","1. In this lab we use greedy search for decoding, that is, always taking the most probable word at current timestep as prediction. Describe an alternative decoding method that might work better than greedy search. You don't have to implement it.\n","\n","2. Pick some samples from dev or test set and visualize their attention maps. Discuss your findings. Hint: compute the attention scores on the input words for each timestep during decoding.\n","\n","3. Compare the performance of seq2seq with and without attention on sentences of different lengths. You can set some length intervals (e.g., 1-10, 11-20, 21-30, 31-40, 41-50) and compare the two models' performance within each length interval. Discuss your findings.\n","\n","4. Try to improve your BLEU score. For example, try stacking more RNN layers, switching cell types, or applying bi-direction to encoder. Describe what you try, even if they don't show improvement. Hints:\n","  * TA's preliminary implemtation of seq2seq with attention model achieves around 16. You don't have to surpass it (although it's pretty simple to do so)--this number is just to give you some sense of what a baseline should get.\n","  * Training on the entire training set takes some time. So tune your hyperparameters on a smaller training set (you can do so by changing `sampling` when creating the data loader)."]}]}