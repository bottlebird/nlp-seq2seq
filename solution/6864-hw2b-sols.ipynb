{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6864-hw2b-sols.ipynb","provenance":[{"file_id":"https://github.com/lingo-mit/6864-hw1/blob/master/6864_hw1.ipynb","timestamp":1582578361533}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FU7xWiY6TyWS","colab_type":"code","colab":{}},"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n","rm -rf 6864-hw2b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5AyMA9rK1Rhf","colab_type":"code","colab":{}},"source":["import os\n","os.makedirs(\"6864-hw2b\", exist_ok=True)\n","import sys\n","sys.path.append(\"/content/6864-hw2b\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BL1IfnRdPdsl","colab_type":"code","outputId":"95229bde-bb98-4372-eed7-58ab57435a21","executionInfo":{"status":"ok","timestamp":1586656262130,"user_tz":240,"elapsed":10658,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!pip install sacrebleu"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting sacrebleu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/58/5c6cc352ea6271125325950715cf8b59b77abe5e93cf29f6e60b491a31d9/sacrebleu-1.4.6-py3-none-any.whl (59kB)\n","\u001b[K     |████████████████████████████████| 61kB 1.1MB/s \n","\u001b[?25hCollecting mecab-python3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n","\u001b[K     |████████████████████████████████| 17.1MB 199kB/s \n","\u001b[?25hCollecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/64/03/9abfb3374d67838daf24f1a388528714bec1debb1d13749f0abd7fb07cfb/portalocker-1.6.0-py2.py3-none-any.whl\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n","Installing collected packages: mecab-python3, portalocker, sacrebleu\n","Successfully installed mecab-python3-0.996.5 portalocker-1.6.0 sacrebleu-1.4.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5fOArV2r9Piz","colab_type":"text"},"source":["# **Part 3 Encoder-Decoder Model**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mA9JfWiK9eoL","colab_type":"text"},"source":["In this lab, you will explore seq2seq with and without attention to perform machine translation task. The main task is to implement the naive encoder-decoder architecture. The dataset we used here is IWSLT'15 English-Vietnamese translation data. \n","\n","To complete this lab, you need to first understand the recurrent update mechanism and the attention mechanism introduced in lecture, and transform them into PyTorch code.\n","\n","In machine translation task(MT), your goal is to translate the correct sentence meaning into another language. Here in our case, we want to translate English sentences into Vietnamese sentences.\n","\n","First, we want to download the dataset."]},{"cell_type":"code","metadata":{"id":"02RioHPryvOz","colab_type":"code","outputId":"f7a136cb-f4b2-4e96-ffd5-91af23a1d3dc","executionInfo":{"status":"ok","timestamp":1586656297355,"user_tz":240,"elapsed":42881,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# Download data\n","!wget -nv -O /content/6864-hw2b/train.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n","!wget -nv -O /content/6864-hw2b/train.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n","!wget -nv -O /content/6864-hw2b/tst2013.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n","!wget -nv -O /content/6864-hw2b/tst2013.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n","!wget -nv -O /content/6864-hw2b/vocab.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n","!wget -nv -O /content/6864-hw2b/vocab.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"],"execution_count":5,"outputs":[{"output_type":"stream","text":["2020-04-12 01:51:06 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en [13603614/13603614] -> \"/content/6864-hw2b/train.en\" [1]\n","2020-04-12 01:51:22 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi [18074646/18074646] -> \"/content/6864-hw2b/train.vi\" [1]\n","2020-04-12 01:51:26 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en [132264/132264] -> \"/content/6864-hw2b/tst2013.en\" [1]\n","2020-04-12 01:51:29 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi [183855/183855] -> \"/content/6864-hw2b/tst2013.vi\" [1]\n","2020-04-12 01:51:32 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/6864-hw2b/vocab.en\" [1]\n","2020-04-12 01:51:35 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi [46767/46767] -> \"/content/6864-hw2b/vocab.vi\" [1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ogFESHAf-6MY","colab_type":"text"},"source":["Here we're going to load the data."]},{"cell_type":"code","metadata":{"id":"OfkQGqV30hgC","colab_type":"code","colab":{}},"source":["def read_sentence_file(filename):\n","  sentences_list = []\n","  with open(filename, \"r\") as f:\n","    for line in f:\n","      sentences_list.append(line.strip().split())\n","  return sentences_list\n","\n","def read_vocab_file(filename):\n","  with open(filename, \"r\") as f:\n","    return [line.strip() for line in f]\n","\n","\n","src_vocab_set = read_vocab_file(os.path.join(\"/content/6864-hw2b\", \"vocab.vi\"))\n","trg_vocab_set = read_vocab_file(os.path.join(\"/content/6864-hw2b\", \"vocab.en\"))\n","\n","train_src_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                           \"train.vi\"))\n","train_trg_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                           \"train.en\"))\n","assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n","\n","test_src_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                          \"tst2013.vi\"))\n","test_trg_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n","                                                          \"tst2013.en\"))\n","assert len(test_src_sentences_list) == len(test_trg_sentences_list)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQ8AeHe7_H7I","colab_type":"text"},"source":["Next, we do preprocessing on the dataset and did a preliminary data analysis."]},{"cell_type":"code","metadata":{"id":"mFjDe3mz2TTE","colab_type":"code","outputId":"17ee42d1-d971-4e87-d96d-cd4041bc5a71","executionInfo":{"status":"ok","timestamp":1586656299509,"user_tz":240,"elapsed":40151,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":472}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","MAX_SENT_LENGTH = 48\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n","\n","# We only keep sentences that do not exceed 48 words, so that later when we\n","# add <s> and </s> to a sentence it still won't exceed 50 words.\n","def filter_data(src_sentences_list, trg_sentences_list, max_len):\n","  new_src_sentences_list, new_trg_sentences_list = [], []\n","  for src_sent, trg_sent in zip(src_sentences_list, trg_sentences_list):\n","    if (len(src_sent) <= max_len and len(trg_sent) <= max_len\n","        and len(src_sent) > 0 and len(trg_sent)) > 0:\n","      new_src_sentences_list.append(src_sent)\n","      new_trg_sentences_list.append(trg_sent)\n","  return new_src_sentences_list, new_trg_sentences_list\n","\n","train_src_sentences_list, train_trg_sentences_list = filter_data(\n","    train_src_sentences_list, train_trg_sentences_list, max_len=MAX_SENT_LENGTH)\n","test_src_sentences_list, test_trg_sentences_list = filter_data(\n","    test_src_sentences_list, test_trg_sentences_list, max_len=MAX_SENT_LENGTH)\n","\n","# We take 10% of training data as validation set.\n","num_val = int(len(train_src_sentences_list) * 0.1)\n","val_src_sentences_list = train_src_sentences_list[:num_val]\n","val_trg_sentences_list = train_trg_sentences_list[:num_val]\n","train_src_sentences_list = train_src_sentences_list[num_val:]\n","train_trg_sentences_list = train_trg_sentences_list[num_val:]\n","\n","# Show some data stats\n","print(\"Number of training (src, trg) sentence pairs: %d\" %\n","      len(train_src_sentences_list))\n","print(\"Number of validation (src, trg) sentence pairs: %d\" %\n","      len(val_src_sentences_list))\n","print(\"Number of testing (src, trg) sentence pairs: %d\" %\n","      len(test_src_sentences_list))\n","src_vocab_set = ['<pad>'] + src_vocab_set\n","trg_vocab_set = ['<pad>'] + trg_vocab_set\n","print(\"Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\" %\n","      len(src_vocab_set))\n","print(\"Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\" %\n","      len(trg_vocab_set))\n","\n","length = [len(sent) for sent in train_src_sentences_list]\n","print('Training sentence avg. length: %d ' % np.mean(length))\n","print('Training sentence length at 95-percentile: %d' %\n","      np.percentile(length, 95))\n","print('Training sentence length distribution '\n","      '(x-axis is length range and y-axis is count):\\n')\n","plt.hist(length, bins=5)\n","plt.show()\n","\n","print('Example Vietnamese input: ' + str(train_src_sentences_list[0]))\n","print('Its target English output: ' + str(train_trg_sentences_list[0]))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Number of training (src, trg) sentence pairs: 108748\n","Number of validation (src, trg) sentence pairs: 12083\n","Number of testing (src, trg) sentence pairs: 1139\n","Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 7710\n","Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 17192\n","Training sentence avg. length: 20 \n","Training sentence length at 95-percentile: 42\n","Training sentence length distribution (x-axis is length range and y-axis is count):\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Example Vietnamese input: ['Adam', 'Sadowsky', 'dàn', 'dựng', '1', 'video', 'âm', 'nhạc', 'hiện', 'tượng', '.']\n","Its target English output: ['Adam', 'Sadowsky', ':', 'How', 'to', 'engineer', 'a', 'viral', 'music', 'video']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2XzArGvi_P3x","colab_type":"text"},"source":["Define the regular torch hyperparameters."]},{"cell_type":"code","metadata":{"id":"L5q_RAT2-Qt1","colab_type":"code","colab":{}},"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","assert device == \"cuda\"   # use gpu whenever you can!\n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2x0lhVm_Yxx","colab_type":"text"},"source":["Load MT dataset into the dataset class. We will apply the dataloader function in pytorch to sample our data later."]},{"cell_type":"code","metadata":{"id":"muwDBzXM5ijT","colab_type":"code","colab":{}},"source":["from torch.utils import data\n","\n","\n","# These IDs are reserved.\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","\n","\n","class MTDataset(data.Dataset):\n","  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n","               sampling=1.):\n","    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n","    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n","\n","    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","\n","    self.src_vocabs = src_vocabs\n","    self.trg_vocabs = trg_vocabs\n","\n","    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n","    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n","    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n","    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n","\n","  def __len__(self):\n","    return len(self.src_sentences)\n","\n","  def __getitem__(self, index):\n","    src_sent = self.src_sentences[index]\n","    src_len = len(src_sent) + 2   # add <s> and </s> to each sentence\n","    src_id = []\n","    for w in src_sent:\n","      if w not in self.src_vocabs:\n","        w = '<unk>'\n","      src_id.append(self.src_v2id[w])\n","    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n","              (self.max_src_seq_length - src_len))\n","\n","    trg_sent = self.trg_sentences[index]\n","    trg_len = len(trg_sent) + 2\n","    trg_id = []\n","    for w in trg_sent:\n","      if w not in self.trg_vocabs:\n","        w = '<unk>'\n","      trg_id.append(self.trg_v2id[w])\n","    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n","              (self.max_trg_seq_length - trg_len))\n","\n","    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kb5gQEp7oVdi","colab_type":"text"},"source":["## Encoder\n","\n","We will start with a single-layer unidirectional GRU. You are free to try bidirectional and stack more layers."]},{"cell_type":"code","metadata":{"id":"dnwVGDVkoPt0","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","    Inputs: \n","      - `input_size`: an int representing the RNN input size.\n","      - `hidden_size`: an int representing the RNN hidden size.\n","      - `dropout`: a float representing the dropout rate during training. Note\n","          that for 1-layer RNN this has no effect since dropout only applies to\n","          outputs of intermediate layers.\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n","                      dropout=dropout, bidirectional=False)\n","\n","  def forward(self, inputs, lengths):\n","    \"\"\"\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of source\n","          sentences.\n","      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n","          lengths of `inputs`.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","        (batch_size, max_seq_length, hidden_size).\n","      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n","      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n","      https://pytorch.org/docs/stable/nn.html#gru\n","    \"\"\"\n","    packed = pack_padded_sequence(inputs, lengths, batch_first=True,\n","                                  enforce_sorted=False)\n","    outputs, finals = self.rnn(packed) #outputs[128,50,256], finals[1, 128, 256]\n","    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n","                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    \n","    return outputs, finals"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYT0BlfYUJXj","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","  \"\"\"An RNN decoder without attention.\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(Decoder, self).__init__()\n","\n","    self.rnn = nn.GRU(input_size, hidden_size, batch_first=True,\n","                      dropout=dropout)\n","\n","    # To initialize from the final encoder state.\n","    self.bridge = nn.Linear(hidden_size, hidden_size, bias=True)\n","\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.pre_output_layer = nn.Linear(hidden_size + input_size, hidden_size,\n","                                      bias=False)\n","\n","  def forward_step(self, prev_embed, hidden):\n","    \"\"\"Perform a single decoder step (1 word).\"\"\"\n","\n","    # Update RNN hidden state.\n","    output, hidden = self.rnn(prev_embed, hidden)\n","\n","    pre_output = torch.cat([prev_embed, output], dim=2)\n","\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.pre_output_layer(pre_output)\n","\n","    return output, hidden, pre_output\n","    \n","  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","\n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of target\n","          sentences (for teacher-forcing during training).\n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `outputs`: a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the raw\n","          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n","    \"\"\"\n","\n","    # The maximum number of steps to unroll the RNN.\n","    if max_len is None:\n","      max_len = inputs.size(1)\n","\n","    # Initialize decoder hidden state.\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    # Here we store all intermediate hidden states and pre-output vectors.\n","    decoder_states = []\n","    pre_output_vectors = []\n","\n","    # Unroll the decoder RNN for `max_len` steps.\n","    for i in range(max_len):\n","      prev_embed = inputs[:, i].unsqueeze(1) #[128,1,256]\n","      output, hidden, pre_output = self.forward_step(prev_embed, hidden)\n","      decoder_states.append(output)\n","      pre_output_vectors.append(pre_output)\n","\n","    decoder_states = torch.cat(decoder_states, dim=1)\n","    pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n","    return hidden, pre_output_vectors\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","    state.\"\"\"\n","    \n","    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n","    #decoder_init_hiddens [1,128,256]\n","    return decoder_init_hiddens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AH0VdHE2_x1k","colab_type":"text"},"source":["Define the high level encoder-decoder class to wrap up sub-models, including encoder, decoder, generator, and src/trg embeddings."]},{"cell_type":"code","metadata":{"id":"nNBaAYB_oHxG","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","  \"\"\"A standard Encoder-Decoder architecture without attention.\n","  \"\"\"\n","  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n","    \"\"\"\n","    Inputs:\n","      - `encoder`: an `Encoder` object.\n","      - `decoder`: an `Decoder` object.\n","      - `src_embed`: an nn.Embedding object representing the lookup table for\n","          input (source) sentences.\n","      - `trg_embed`: an nn.Embedding object representing the lookup table for\n","          output (target) sentences.\n","      - `generator`: a `Generator` object. Essentially a linear mapping. See\n","          the next code cell.\n","    \"\"\"\n","    super(EncoderDecoder, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.trg_embed = trg_embed\n","    self.generator = generator\n","\n","  def forward(self, src_ids, trg_ids, src_lengths):\n","    \"\"\"Take in and process masked source and target sequences.\n","\n","    Inputs:\n","      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of source sentences of word ids.\n","      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of target sentences of word ids.\n","      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n","        sequence length of `src_ids`.\n","\n","    Returns the decoder outputs, see the above cell.\n","    \"\"\"\n","    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n","    del encoder_hiddens\n","    return self.decode(encoder_finals, trg_ids[:, :-1])\n","    #trg_ids[:,:-1] (128,49)\n","\n","  def encode(self, src_ids, src_lengths):\n","    return self.encoder(self.src_embed(src_ids), src_lengths)\n","    \n","  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n","    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)\n","    #trg_embed(trg_ids) (128,49,256)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhV4XbsEYPtd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"72119b1f-4c5f-493e-faf9-66d6bf21f0a2","executionInfo":{"status":"ok","timestamp":1586658062027,"user_tz":240,"elapsed":1468,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}}},"source":["src_embed=nn.Embedding(len(src_vocab_set), 256)\n","trg_embed=nn.Embedding(len(trg_vocab_set), 256)\n","\n","src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B = next(iter(train_data_loader))\n","print(len(train_data_loader))\n","print(trg_ids_BxL.size())\n","\n","print(\"input size:\",trg_embed(trg_ids_BxL[:,:-1]).size())\n","inputs = trg_embed(trg_ids_BxL[:,:-1])\n","prev_embed = inputs[:, 1].unsqueeze(0).size()\n","print(prev_embed)\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["850\n","torch.Size([128, 50])\n","input size: torch.Size([128, 49, 256])\n","torch.Size([128, 1, 256])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M06QOTbCALGy","colab_type":"text"},"source":["It simply projects the pre-output layer (x in the forward function below) to obtain the output layer, so that the final dimension is the target vocabulary size."]},{"cell_type":"code","metadata":{"id":"LaHdVcF1KPmd","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","  \"\"\"Define standard linear + softmax generation step.\"\"\"\n","  def __init__(self, hidden_size, vocab_size):\n","    super(Generator, self).__init__()\n","    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","  def forward(self, x):\n","    return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qf8Oc9a_ocC_","colab_type":"text"},"source":["### Attention-based Decoder\n","\n","The decoder is a conditional GRU. Rather than starting with an empty state like the encoder, its initial hidden state results from a projection of the encoder final vector. \n"]},{"cell_type":"code","metadata":{"id":"iZq2NImAoY1C","colab_type":"code","colab":{}},"source":["class AttentionDecoder(nn.Module):\n","  \"\"\"An attention-based RNN decoder.\"\"\"\n","\n","  def __init__(self, input_size, hidden_size, attention, dropout=0.):\n","    \"\"\"\n","      Inputs:\n","        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n","    \"\"\"\n","    super(AttentionDecoder, self).__init__()\n","\n","    self.attention = attention\n","\n","    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, batch_first=True,\n","                      dropout=dropout)\n","\n","    # To initialize from the final encoder state.\n","    self.bridge = nn.Linear(hidden_size, hidden_size, bias=True)\n","\n","    self.dropout_layer = nn.Dropout(p=dropout)\n","    self.pre_output_layer = nn.Linear(hidden_size + hidden_size + input_size,\n","                                      hidden_size, bias=False)\n","\n","  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key,\n","                   hidden):\n","    \"\"\"Perform a single decoder step (1 word)\"\"\"\n","\n","    # Compute context vector using attention mechanism.\n","    query = hidden[-1].unsqueeze(1)\n","\n","    context, attn_probs = self.attention(query=query, proj_key=proj_key,\n","                                         value=encoder_hidden, mask=src_mask)\n","\n","    # Update RNN hidden state.\n","    rnn_input = torch.cat([prev_embed, context], dim=2)\n","    output, hidden = self.rnn(rnn_input, hidden)\n","\n","    pre_output = torch.cat([prev_embed, output, context], dim=2)\n","\n","    pre_output = self.dropout_layer(pre_output)\n","    pre_output = self.pre_output_layer(pre_output)\n","\n","    return output, hidden, pre_output\n","    \n","  def forward(self, inputs, encoder_hiddens, encoder_finals,  src_mask,\n","              trg_mask, hidden=None, max_len=None):\n","    \"\"\"Unroll the decoder one step at a time.\n","    \n","    Inputs:\n","      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n","          representing a batch of padded embedded word vectors of target\n","          sentences (for teacher-forcing during training).\n","      - `encoder_hiddens`: a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the encoder\n","          outputs for each decoding step to attend to. \n","      - `encoder_finals`: a 3d-tensor of shape\n","          (num_enc_layers, batch_size, hidden_size) representing the final\n","          encoder hidden states used to initialize the initial decoder hidden\n","          states.\n","      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n","          representing the mask for source sentences.\n","      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n","          representing the mask for target sentences.\n","      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n","          the value to be used to initialize the initial decoder hidden states.\n","          If None, then use `encoder_finals`.\n","      - `max_len`: an int representing the maximum decoding length.\n","\n","    Returns:\n","      - `outputs`: (same as in Decoder) a 3d-tensor of shape\n","          (batch_size, max_seq_length, hidden_size) representing the raw\n","          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n","    \"\"\"\n","\n","    # The maximum number of steps to unroll the RNN.\n","    if max_len is None:\n","      max_len = trg_mask.size(-1)\n","\n","    if hidden is None:\n","      hidden = self.init_hidden(encoder_finals)\n","\n","    # Pre-compute projected encoder hidden states (the \"keys\" for the attention\n","    # mechanism). This is only done for efficiency.\n","    proj_key = self.attention.key_layer(encoder_hiddens)\n","\n","    # Here we store all intermediate hidden states and pre-output vectors.\n","    decoder_states = []\n","    pre_output_vectors = []\n","\n","    # Unroll the decoder RNN for `max_len` steps.\n","    for i in range(max_len):\n","      prev_embed = inputs[:, i].unsqueeze(1)\n","      output, hidden, pre_output = self.forward_step(prev_embed,\n","                                                     encoder_hiddens, src_mask,\n","                                                     proj_key, hidden)\n","      decoder_states.append(output)\n","      pre_output_vectors.append(pre_output)\n","\n","    decoder_states = torch.cat(decoder_states, dim=1)\n","    pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n","    return hidden, pre_output_vectors\n","\n","  def init_hidden(self, encoder_finals):\n","    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n","    state.\"\"\"\n","    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n","    return decoder_init_hiddens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mghIa6XzubZL","colab_type":"code","colab":{}},"source":["class EncoderAttentionDecoder(nn.Module):\n","  \"\"\"A Encoder-Decoder architecture with attention.\n","  \"\"\"\n","  def __init__(self, encoder, decoder, src_embed , trg_embed, generator):\n","    \"\"\"\n","    Inputs:\n","      - `encoder`: an `Encoder` object.\n","      - `decoder`: an `AttentionDecoder` object.\n","      - `src_embed`: an nn.Embedding object representing the lookup table for\n","          input (source) sentences.\n","      - `trg_embed`: an nn.Embedding object representing the lookup table for\n","          output (target) sentences.\n","      - `generator`: a `Generator` object. Essentially a linear mapping. See\n","          the next code cell.\n","    \"\"\"\n","    super(EncoderAttentionDecoder, self).__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.trg_embed = trg_embed\n","    self.generator = generator\n","\n","  def forward(self, src_ids, trg_ids, src_lengths):\n","    \"\"\"Take in and process masked source and tar get sequences.\n","\n","    Inputs:\n","      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of source sentences of word ids.\n","      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n","        a batch of target sentences of word ids.\n","      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n","        sequence length of `src_ids`.\n","\n","    Returns the decoder outputs, see the above cell.\n","    \"\"\"\n","    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n","    src_mask = (src_ids != PAD_INDEX).unsqueeze(-2)\n","    trg_mask = (trg_ids[:, 1:] != PAD_INDEX).unsqueeze(-2)\n","    return self.decode(encoder_hiddens, encoder_finals, src_mask,\n","                       trg_ids[:, :-1], trg_mask)\n","\n","  def encode(self, src_ids, src_lengths):\n","    return self.encoder(self.src_embed(src_ids), src_lengths)\n","\n","  def decode(self, encoder_hiddens, encoder_finals, src_mask, trg_ids,\n","             trg_mask, decoder_hidden=None):\n","    return self.decoder(self.trg_embed(trg_ids), encoder_hiddens,\n","                        encoder_finals, src_mask, trg_mask, decoder_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLUt0XK_oo06","colab_type":"text"},"source":["### Attention                     "]},{"cell_type":"markdown","metadata":{"id":"9GoiVH9TAabX","colab_type":"text"},"source":["Attention layer plays a key role in seq2seq models. It allows for capturing the dependencies without the distance constraints in the input or output sequences. In this lab, your goal is to implement the Bahdanau attention mechanisms. "]},{"cell_type":"code","metadata":{"id":"LIAkZUwEolVk","colab_type":"code","colab":{}},"source":["class BahdanauAttention(nn.Module):\n","  \"\"\"Implements Bahdanau (MLP) attention.\"\"\"\n","\n","  def __init__(self, hidden_size, key_size=None, query_size=None):\n","    super(BahdanauAttention, self).__init__()\n","\n","    key_size = hidden_size if key_size is None else key_size\n","    query_size = hidden_size if query_size is None else query_size\n","\n","    self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","    self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","    self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","\n","    # To store attention scores.\n","    self.alphas = None\n","        \n","  def forward(self, query=None, proj_key=None, value=None, mask=None):\n","    assert mask is not None, \"mask is required\"\n","\n","    # We first project the query (the decoder state).\n","    # The projected keys (the encoder states) were already pre-computated.\n","    query = self.query_layer(query)\n","\n","    # Calculate scores.\n","    scores = self.energy_layer(torch.tanh(query + proj_key))\n","    scores = scores.squeeze(2).unsqueeze(1)\n","\n","    # Mask out invalid positions.\n","    # The mask marks valid positions so we invert it using `mask & 0`.\n","    scores.data.masked_fill_(mask == 0, -float('inf'))\n","\n","    # Turn scores to probabilities.\n","    alphas = F.softmax(scores, dim=-1)\n","    self.alphas = alphas        \n","\n","    # The context vector is the weighted sum of the values.\n","    context = torch.bmm(alphas, value)\n","\n","    # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","    return context, alphas"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I38IFq48BKa5","colab_type":"text"},"source":["Apply the dataloader to the MT dataset. Dataloader provides a convenient way to iterate through the whole dataset."]},{"cell_type":"code","metadata":{"id":"cJrXO7nCjzBP","colab_type":"code","colab":{}},"source":["batch_size = 128\n","\n","# You can try on a smaller training set by setting a smaller `sampling`.\n","train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n","                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n","train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n","                                    num_workers=8, shuffle=True)\n","\n","val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n","                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n","val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n","                                  shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWaiu7wNBX7x","colab_type":"text"},"source":["The main functions for training, here we use perplexity to evaluate the performance of the model.\n","\n","Although we provide the training scripts here, we strongly encoureage you to go through and understand the procedure."]},{"cell_type":"code","metadata":{"id":"AXGa-L1qp13q","colab_type":"code","colab":{}},"source":["import math\n","\n","\n","class SimpleLossCompute:\n","  \"\"\"A simple loss compute and train function.\"\"\"\n","\n","  def __init__(self, generator, criterion, opt=None):\n","    self.generator = generator\n","    self.criterion = criterion\n","    self.opt = opt\n","\n","  def __call__(self, x, y, norm):\n","    x = self.generator(x)\n","    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                          y.contiguous().view(-1))\n","    loss = loss / norm\n","\n","    if self.opt is not None:  # training mode\n","      loss.backward()          \n","      self.opt.step()\n","      self.opt.zero_grad()\n","\n","    return loss.data.item() * norm\n","\n","\n","def run_epoch(data_loader, model, loss_compute, print_every):\n","  \"\"\"Standard Training and Logging Function\"\"\"\n","\n","  total_tokens = 0\n","  total_loss = 0\n","\n","  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n","    # We define some notations here to help you understand the loaded tensor\n","    # shapes:\n","    #   `B`: batch size\n","    #   `T`: max sequence length of source sentences\n","    #   `L`: max sequence length of target sentences; due to our preprocessing\n","    #        in the beginning, `L` == `T` == 50\n","    # An example of `src_ids_BxT` (when B = 2):\n","    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n","    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n","    # The corresponding `src_lengths_B` would be [47, 49].\n","    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n","\n","    src_ids_BxT = src_ids_BxT.to(device)\n","    src_lengths_B = src_lengths_B.to(device)\n","    trg_ids_BxL = trg_ids_BxL.to(device)\n","    del trg_lengths_B   # unused\n","\n","    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n","\n","    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n","                        norm=src_ids_BxT.size(0))\n","    total_loss += loss\n","    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n","\n","  return math.exp(total_loss / float(total_tokens))\n","\n","\n","def train(model, num_epochs, learning_rate, print_every):\n","  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","  # computing the loss.\n","  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n","  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # Keep track of dev ppl for each epoch.\n","  dev_ppls = []\n","\n","  for epoch in range(num_epochs):\n","    print(\"Epoch\", epoch)\n","\n","    model.train()\n","    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n","                          loss_compute=SimpleLossCompute(model.generator,\n","                                                         criterion, optim),\n","                          print_every=print_every)\n","\n","    model.eval()\n","    with torch.no_grad():      \n","      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n","                          loss_compute=SimpleLossCompute(model.generator,\n","                                                         criterion, None),\n","                          print_every=print_every)\n","      print(\"Validation perplexity: %f\" % dev_ppl)\n","      dev_ppls.append(dev_ppl)\n","        \n","  return dev_ppls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1A8VvkcICT60","colab_type":"text"},"source":["The main function to perform training."]},{"cell_type":"code","metadata":{"id":"pZ0t1hXAIHtO","colab_type":"code","outputId":"9d3330f8-9f37-43f9-a6a8-83c35da36948","executionInfo":{"status":"error","timestamp":1586641280682,"user_tz":240,"elapsed":14555,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Hyperparameters for contructing the encoder-decoder model.\n","\n","embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n","hidden_size = 256  # RNN hidden size.\n","dropout = 0.2\n","\n","pure_seq2seq = EncoderDecoder(\n","  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n","  decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n","  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n","  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n","  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n","\n","# Start training. The returned `dev_ppls` is a list of dev perplexity for each\n","# epoch.\n","pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n","                      print_every=100)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","Epoch Step: 0 Loss: 164.283936\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n","finals size torch.Size([1, 128, 256])\n","encoder_finals torch.Size([1, 128, 256])\n","initialized encoder_finals torch.Size([1, 128, 256])\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-ffac4e7ef14a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n\u001b[0;32m---> 16\u001b[0;31m                       print_every=100)\n\u001b[0m","\u001b[0;32m<ipython-input-14-a6d6580d705d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, learning_rate, print_every)\u001b[0m\n\u001b[1;32m     77\u001b[0m                           loss_compute=SimpleLossCompute(model.generator,\n\u001b[1;32m     78\u001b[0m                                                          criterion, optim),\n\u001b[0;32m---> 79\u001b[0;31m                           print_every=print_every)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-a6d6580d705d>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_loader, model, loss_compute, print_every)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n\u001b[0;32m---> 53\u001b[0;31m                         norm=src_ids_BxT.size(0))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrg_ids_BxL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPAD_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-a6d6580d705d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"fRsfDg0wCa7U","colab_type":"text"},"source":["Plot the perplexity graph."]},{"cell_type":"code","metadata":{"id":"CTApnlT53YvT","colab_type":"code","outputId":"f7d311d2-c331-4e4d-b133-e0d83c10174e","executionInfo":{"status":"ok","timestamp":1583260983095,"user_tz":300,"elapsed":892183,"user":{"displayName":"Yu-An Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-GERovpgLuP2lhtg56j-6mhPX3jvv-703ftKoFg=s64","userId":"02705261175896611275"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["def plot_perplexity(perplexities):\n","  \"\"\"plot perplexities\"\"\"\n","  plt.title(\"Perplexity per Epoch\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Perplexity\")\n","  plt.plot(perplexities)\n","\n","plot_perplexity(pure_dev_ppls)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"onQxDU-aGa0t","colab_type":"code","outputId":"399624fa-9d19-435d-a2f2-5988692b7ae5","executionInfo":{"status":"ok","timestamp":1583262304820,"user_tz":300,"elapsed":2213889,"user":{"displayName":"Yu-An Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-GERovpgLuP2lhtg56j-6mhPX3jvv-703ftKoFg=s64","userId":"02705261175896611275"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["attn_seq2seq = EncoderAttentionDecoder(\n","  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n","  decoder=AttentionDecoder(embed_size, hidden_size,\n","                  attention=BahdanauAttention(hidden_size), dropout=dropout),\n","  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n","  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n","  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n","\n","attn_dev_ppls = train(attn_seq2seq, num_epochs=10, learning_rate=1e-3,\n","                      print_every=100)\n","\n","plot_perplexity(attn_dev_ppls)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch Step: 0 Loss: 180.053177\n","Epoch Step: 100 Loss: 95.803543\n","Epoch Step: 200 Loss: 88.990448\n","Epoch Step: 300 Loss: 73.352814\n","Epoch Step: 400 Loss: 74.548058\n","Epoch Step: 500 Loss: 67.995300\n","Epoch Step: 600 Loss: 68.646118\n","Epoch Step: 700 Loss: 69.664398\n","Epoch Step: 800 Loss: 66.449677\n","Validation perplexity: 32.856711\n","Epoch 1\n","Epoch Step: 0 Loss: 61.769150\n","Epoch Step: 100 Loss: 58.377026\n","Epoch Step: 200 Loss: 55.020344\n","Epoch Step: 300 Loss: 56.482159\n","Epoch Step: 400 Loss: 57.452576\n","Epoch Step: 500 Loss: 61.371292\n","Epoch Step: 600 Loss: 57.677532\n","Epoch Step: 700 Loss: 55.710712\n","Epoch Step: 800 Loss: 54.746990\n","Validation perplexity: 20.907627\n","Epoch 2\n","Epoch Step: 0 Loss: 51.143478\n","Epoch Step: 100 Loss: 47.702641\n","Epoch Step: 200 Loss: 49.896042\n","Epoch Step: 300 Loss: 50.827049\n","Epoch Step: 400 Loss: 48.907604\n","Epoch Step: 500 Loss: 47.258926\n","Epoch Step: 600 Loss: 46.769863\n","Epoch Step: 700 Loss: 49.825222\n","Epoch Step: 800 Loss: 42.675018\n","Validation perplexity: 17.617843\n","Epoch 3\n","Epoch Step: 0 Loss: 41.035057\n","Epoch Step: 100 Loss: 43.213551\n","Epoch Step: 200 Loss: 42.505852\n","Epoch Step: 300 Loss: 39.403046\n","Epoch Step: 400 Loss: 41.849926\n","Epoch Step: 500 Loss: 48.377953\n","Epoch Step: 600 Loss: 45.159416\n","Epoch Step: 700 Loss: 43.309181\n","Epoch Step: 800 Loss: 46.552601\n","Validation perplexity: 16.462851\n","Epoch 4\n","Epoch Step: 0 Loss: 38.858662\n","Epoch Step: 100 Loss: 39.851494\n","Epoch Step: 200 Loss: 38.311249\n","Epoch Step: 300 Loss: 38.220779\n","Epoch Step: 400 Loss: 40.642601\n","Epoch Step: 500 Loss: 41.699070\n","Epoch Step: 600 Loss: 34.720135\n","Epoch Step: 700 Loss: 40.750305\n","Epoch Step: 800 Loss: 40.642513\n","Validation perplexity: 16.178031\n","Epoch 5\n","Epoch Step: 0 Loss: 35.921467\n","Epoch Step: 100 Loss: 42.307655\n","Epoch Step: 200 Loss: 37.884918\n","Epoch Step: 300 Loss: 38.587528\n","Epoch Step: 400 Loss: 40.771347\n","Epoch Step: 500 Loss: 37.038982\n","Epoch Step: 600 Loss: 37.059120\n","Epoch Step: 700 Loss: 40.287537\n","Epoch Step: 800 Loss: 38.345669\n","Validation perplexity: 16.081105\n","Epoch 6\n","Epoch Step: 0 Loss: 33.197666\n","Epoch Step: 100 Loss: 33.950863\n","Epoch Step: 200 Loss: 34.660889\n","Epoch Step: 300 Loss: 36.808357\n","Epoch Step: 400 Loss: 33.776340\n","Epoch Step: 500 Loss: 37.736893\n","Epoch Step: 600 Loss: 34.605076\n","Epoch Step: 700 Loss: 35.992680\n","Epoch Step: 800 Loss: 37.552654\n","Validation perplexity: 16.288891\n","Epoch 7\n","Epoch Step: 0 Loss: 32.309101\n","Epoch Step: 100 Loss: 35.427147\n","Epoch Step: 200 Loss: 35.219196\n","Epoch Step: 300 Loss: 33.005783\n","Epoch Step: 400 Loss: 35.363674\n","Epoch Step: 500 Loss: 34.556347\n","Epoch Step: 600 Loss: 36.545673\n","Epoch Step: 700 Loss: 34.148689\n","Epoch Step: 800 Loss: 37.999687\n","Validation perplexity: 16.793218\n","Epoch 8\n","Epoch Step: 0 Loss: 32.296158\n","Epoch Step: 100 Loss: 32.119167\n","Epoch Step: 200 Loss: 33.337685\n","Epoch Step: 300 Loss: 36.006920\n","Epoch Step: 400 Loss: 33.694439\n","Epoch Step: 500 Loss: 37.400215\n","Epoch Step: 600 Loss: 32.846931\n","Epoch Step: 700 Loss: 32.756725\n","Epoch Step: 800 Loss: 35.432934\n","Validation perplexity: 16.856173\n","Epoch 9\n","Epoch Step: 0 Loss: 34.181019\n","Epoch Step: 100 Loss: 29.658108\n","Epoch Step: 200 Loss: 28.200895\n","Epoch Step: 300 Loss: 32.252594\n","Epoch Step: 400 Loss: 31.279898\n","Epoch Step: 500 Loss: 34.357582\n","Epoch Step: 600 Loss: 34.318638\n","Epoch Step: 700 Loss: 31.352262\n","Epoch Step: 800 Loss: 32.014626\n","Validation perplexity: 17.314031\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTKD+AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"M1HTqYwy6-yL","colab_type":"code","colab":{}},"source":["def greedy_decode(model, src_ids, src_lengths, max_len):\n","  \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n","\n","  with torch.no_grad():\n","    _, encoder_finals = model.encode(src_ids, src_lengths)\n","    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n","\n","  output = []\n","  hidden = None\n","\n","  for i in range(max_len):\n","    with torch.no_grad():\n","      hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n","      prob = model.generator(outputs[:, -1])\n","\n","    _, next_word = torch.max(prob, dim=1)\n","    next_word = next_word.data.item()\n","    output.append(next_word)\n","    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n","\n","  output = np.array(output)\n","\n","  # Cut off everything starting from </s>.\n","  first_eos = np.where(output == EOS_INDEX)[0]\n","  if len(first_eos) > 0:\n","    output = output[:first_eos[0]]\n","  return output\n","\n","\n","def greedy_decode_attention(model, src_ids, src_lengths, max_len):\n","  \"\"\"Greedily decode a sentence for EncoderAttentionDecoder.\"\"\"\n","\n","  with torch.no_grad():\n","    src_mask = (src_ids != PAD_INDEX).unsqueeze(-2)\n","    encoder_hiddens, encoder_finals = model.encode(src_ids, src_lengths)\n","    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n","    trg_mask = torch.ones_like(prev_y)\n","\n","  output = []\n","  attention_scores = []\n","  hidden = None\n","\n","  for i in range(max_len):\n","    with torch.no_grad():\n","      hidden, outputs = model.decode(encoder_hiddens, encoder_finals, src_mask,\n","                                     prev_y, trg_mask, hidden)\n","      prob = model.generator(outputs[:, -1])\n","\n","    _, next_word = torch.max(prob, dim=1)\n","    next_word = next_word.data.item()\n","    output.append(next_word)\n","    prev_y = torch.ones(1, 1).fill_(next_word).type_as(src_ids)\n","    attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n","\n","  output = np.array(output)\n","\n","  # Cut off everything starting from </s>.\n","  first_eos = np.where(output == EOS_INDEX)[0]\n","  if len(first_eos) > 0:\n","    output = output[:first_eos[0]]\n","  return output, np.concatenate(attention_scores, axis=1)\n","  \n","\n","def lookup_words(x, vocab):\n","  return [vocab[i] for i in x]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cc3m4optFrb3","colab_type":"code","colab":{}},"source":["def print_examples(model, data_loader, n=3,\n","                   max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS, \n","                   src_vocab_set=src_vocab_set, trg_vocab_set=trg_vocab_set):\n","  \"\"\"Prints `n` examples. Assumes batch size of 1.\"\"\"\n","\n","  model.eval()\n","\n","  for i, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n","    if isinstance(model, EncoderDecoder):\n","      result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n","                             max_len=max_len)\n","    elif isinstance(model, EncoderAttentionDecoder):\n","      result, _ = greedy_decode_attention(model, src_ids.to(device),\n","                                          src_lengths.to(device),\n","                                          max_len=max_len)\n","    else:\n","      raise NotImplementedError(\"Unknown model type.\")\n","\n","    # remove <s>\n","    src_ids = src_ids[0, 1:]\n","    trg_ids = trg_ids[0, 1:]\n","    # remove </s> and <pad>\n","    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n","    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n","\n","    print(\"Example #%d\" % (i + 1))\n","    print(\"Src : \", \" \".join(lookup_words(src_ids, vocab=src_vocab_set)))\n","    print(\"Trg : \", \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set)))\n","    print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab_set)))\n","    print()\n","\n","    if i == n - 1:\n","      break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"27thJIfreCgB","colab_type":"code","outputId":"5b550b67-3cfa-4cd5-86a1-a579127116f1","executionInfo":{"status":"ok","timestamp":1583262305175,"user_tz":300,"elapsed":2214221,"user":{"displayName":"Yu-An Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-GERovpgLuP2lhtg56j-6mhPX3jvv-703ftKoFg=s64","userId":"02705261175896611275"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n","                        val_trg_sentences_list, trg_vocab_set)\n","example_data_loader = data.DataLoader(val_set, batch_size=1, num_workers=1,\n","                                      shuffle=False)\n","\n","print_examples(pure_seq2seq, example_data_loader)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Example #1\n","Src :  Khoa học đằng sau một tiêu đề về khí hậu\n","Trg :  Rachel <unk> : The science behind a climate headline\n","Pred:  Science has to solve climate crisis .\n","\n","Example #2\n","Src :  Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n","Trg :  I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n","Pred:  I want to tell you what scientists have to do with the most amazing thing about the <unk> of the world .\n","\n","Example #3\n","Src :  Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .\n","Trg :  <unk> that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n","Pred:  There are these things that are going to happen in the air , and if you &apos;re going to be <unk> , it &apos;s not like the <unk> of the air .\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6XGQYwHRPyne","colab_type":"code","outputId":"b91c3ed9-d7b1-4895-cc7a-1a40e3f12915","executionInfo":{"status":"ok","timestamp":1583262479361,"user_tz":300,"elapsed":2388395,"user":{"displayName":"Yu-An Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-GERovpgLuP2lhtg56j-6mhPX3jvv-703ftKoFg=s64","userId":"02705261175896611275"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import sacrebleu\n","from tqdm import tqdm\n","\n","\n","def compute_BLEU(model, data_loader):\n","  bleu_score = []\n","\n","  model.eval()\n","  for src_ids, src_lengths, trg_ids, _ in tqdm(data_loader):\n","    if isinstance(model, EncoderDecoder):\n","      result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n","                             max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    elif isinstance(model, EncoderAttentionDecoder):\n","      result, _ = greedy_decode_attention(model, src_ids.to(device),\n","                                          src_lengths.to(device),\n","                                          max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","    else:\n","      raise NotImplementedError(\"Unknown model type.\")\n","\n","    # remove <s>\n","    src_ids = src_ids[0, 1:]\n","    trg_ids = trg_ids[0, 1:]\n","    # remove </s> and <pad>\n","    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n","    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n","\n","    pred = \" \".join(lookup_words(result, vocab=trg_vocab_set))\n","    targ = \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set))\n","\n","    bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n","\n","  return bleu_score\n","\n","\n","test_set = MTDataset(test_src_sentences_list, src_vocab_set,\n","                     test_trg_sentences_list, trg_vocab_set, sampling=1.)\n","test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers=8,\n","                                   shuffle=False)\n","\n","print(np.mean(compute_BLEU(pure_seq2seq, test_data_loader)))  # BLEU: 5.8\n","print(np.mean(compute_BLEU(attn_seq2seq, test_data_loader)))  # BLEU: 14.9"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 1139/1139 [01:11<00:00, 15.62it/s]\n","  0%|          | 0/1139 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["5.882031050834033\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1139/1139 [01:42<00:00, 11.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["14.905515370641666\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"GbOgwJw_CkCW","colab_type":"text"},"source":["## **Part 3: Lab writeup**\n","\n","Your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n","\n","1. In this lab we use greedy search for decoding, that is, always taking the most likely word at each timestep as prediction. Describe an alternative decoding method that might work better than greedy search. You don't have to implement it.\n","\n","2. Pick some samples and visualize their attention maps. Discuss your findings. Hint: compute the attention scores on the input words for each timestep when you perform decoding.\n","\n","3. Compare the performance of seq2seq with and without attention on sentences of different lengths. You can set some length intervals (e.g., 1-10, 11-20, 21-30, 31-40, 41-50) and compare the two models's BLEU scores within each length interval. Discuss your findings.\n","\n","4. Try to improve your BLEU score on test set. For example, try stacking more RNN layers, switching cell types, or applying bi-direction to encoder. Hints:\n","  * TA's preliminary implemtation of seq2seq with attention model achieves around 16. You don't have to surpass it (although it's pretty simple to do so)--this number is just to give you some sense of a baseline.\n","  * Training on the entire training set takes some time. So tune your hyperparameters on a smaller training set (you can do so by changing `sampling` when creating the data loader). Most of the time the performance transfer to large data."]}]}